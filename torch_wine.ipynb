{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic_Acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Ash_Alcanity</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total_Phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid_Phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color_Intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280</th>\n",
       "      <th>Proline</th>\n",
       "      <th>Customer_Segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Alcohol  Malic_Acid   Ash  Ash_Alcanity  Magnesium  Total_Phenols  \\\n",
       "0    14.23        1.71  2.43          15.6        127           2.80   \n",
       "1    13.20        1.78  2.14          11.2        100           2.65   \n",
       "2    13.16        2.36  2.67          18.6        101           2.80   \n",
       "3    14.37        1.95  2.50          16.8        113           3.85   \n",
       "4    13.24        2.59  2.87          21.0        118           2.80   \n",
       "\n",
       "   Flavanoids  Nonflavanoid_Phenols  Proanthocyanins  Color_Intensity   Hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   OD280  Proline  Customer_Segment  \n",
       "0   3.92     1065                 1  \n",
       "1   3.40     1050                 1  \n",
       "2   3.17     1185                 1  \n",
       "3   3.45     1480                 1  \n",
       "4   2.93      735                 1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data = pd.read_csv(\"E:/Dataset/Wine.csv\")\n",
    "my_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data['Customer_Segment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings ={\n",
    "    1:0,\n",
    "    2:1,\n",
    "    3:2\n",
    "}\n",
    "\n",
    "my_data['Customer_Segment'] = my_data['Customer_Segment'].apply(lambda x:mappings[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic_Acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Ash_Alcanity</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total_Phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid_Phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color_Intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280</th>\n",
       "      <th>Proline</th>\n",
       "      <th>Customer_Segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Alcohol  Malic_Acid   Ash  Ash_Alcanity  Magnesium  Total_Phenols  \\\n",
       "0    14.23        1.71  2.43          15.6        127           2.80   \n",
       "1    13.20        1.78  2.14          11.2        100           2.65   \n",
       "2    13.16        2.36  2.67          18.6        101           2.80   \n",
       "3    14.37        1.95  2.50          16.8        113           3.85   \n",
       "4    13.24        2.59  2.87          21.0        118           2.80   \n",
       "\n",
       "   Flavanoids  Nonflavanoid_Phenols  Proanthocyanins  Color_Intensity   Hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   OD280  Proline  Customer_Segment  \n",
       "0   3.92     1065                 0  \n",
       "1   3.40     1050                 0  \n",
       "2   3.17     1185                 0  \n",
       "3   3.45     1480                 0  \n",
       "4   2.93      735                 0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = my_data.iloc[:,:-1].values\n",
    "y = my_data.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y,train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(x_train)\n",
    "Y_train = torch.LongTensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = Y_train.view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(network,self).__init__()\n",
    "            self.l1 = nn.Linear(13,26)\n",
    "            self.l2 = nn.Linear(26,52)\n",
    "            self.l3 = nn.Linear(52,13)\n",
    "            self.l4 = nn.Linear(13,3)      \n",
    "            self.relu = nn.ReLU()\n",
    "            \n",
    "        def forward(self,x):\n",
    "                out = self.l1(x)\n",
    "                out = F.relu(out)\n",
    "                out = self.l2(out)\n",
    "                out = F.relu(out)\n",
    "                out = self.l3(out)\n",
    "                out = F.relu(out)\n",
    "                out = self.l4(out)\n",
    "                return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 and loss 0.6251357793807983\n",
      "epoch 2 and loss 0.6299050450325012\n",
      "epoch 3 and loss 0.6352499723434448\n",
      "epoch 4 and loss 0.6368280053138733\n",
      "epoch 5 and loss 0.643747866153717\n",
      "epoch 6 and loss 0.6454730033874512\n",
      "epoch 7 and loss 0.6314675211906433\n",
      "epoch 8 and loss 0.6397202610969543\n",
      "epoch 9 and loss 0.625918447971344\n",
      "epoch 10 and loss 0.6263803839683533\n",
      "epoch 11 and loss 0.6206339001655579\n",
      "epoch 12 and loss 0.6210788488388062\n",
      "epoch 13 and loss 0.6435854434967041\n",
      "epoch 14 and loss 0.6451500654220581\n",
      "epoch 15 and loss 0.6294248700141907\n",
      "epoch 16 and loss 0.6306480169296265\n",
      "epoch 17 and loss 0.6367782950401306\n",
      "epoch 18 and loss 0.6421459913253784\n",
      "epoch 19 and loss 0.6292583346366882\n",
      "epoch 20 and loss 0.6308133602142334\n",
      "epoch 21 and loss 0.6250008344650269\n",
      "epoch 22 and loss 0.6297944784164429\n",
      "epoch 23 and loss 0.6343162655830383\n",
      "epoch 24 and loss 0.6385038495063782\n",
      "epoch 25 and loss 0.6241340637207031\n",
      "epoch 26 and loss 0.6265656352043152\n",
      "epoch 27 and loss 0.6218776702880859\n",
      "epoch 28 and loss 0.6243249177932739\n",
      "epoch 29 and loss 0.6165850162506104\n",
      "epoch 30 and loss 0.6116074919700623\n",
      "epoch 31 and loss 0.6092902421951294\n",
      "epoch 32 and loss 0.6067900061607361\n",
      "epoch 33 and loss 0.6037835478782654\n",
      "epoch 34 and loss 0.6031309366226196\n",
      "epoch 35 and loss 0.6036799550056458\n",
      "epoch 36 and loss 0.6055266261100769\n",
      "epoch 37 and loss 0.6129851937294006\n",
      "epoch 38 and loss 0.6125611662864685\n",
      "epoch 39 and loss 0.61872798204422\n",
      "epoch 40 and loss 0.6215437650680542\n",
      "epoch 41 and loss 0.6466341614723206\n",
      "epoch 42 and loss 0.6498160362243652\n",
      "epoch 43 and loss 0.6722661256790161\n",
      "epoch 44 and loss 0.6406136751174927\n",
      "epoch 45 and loss 0.6327208280563354\n",
      "epoch 46 and loss 0.6309399604797363\n",
      "epoch 47 and loss 0.6150317192077637\n",
      "epoch 48 and loss 0.6143989562988281\n",
      "epoch 49 and loss 0.6105412244796753\n",
      "epoch 50 and loss 0.6080145835876465\n",
      "epoch 51 and loss 0.6050014495849609\n",
      "epoch 52 and loss 0.6041737198829651\n",
      "epoch 53 and loss 0.6033679246902466\n",
      "epoch 54 and loss 0.603157639503479\n",
      "epoch 55 and loss 0.6036852598190308\n",
      "epoch 56 and loss 0.6050097346305847\n",
      "epoch 57 and loss 0.6111857891082764\n",
      "epoch 58 and loss 0.6088517308235168\n",
      "epoch 59 and loss 0.6076785922050476\n",
      "epoch 60 and loss 0.6035788059234619\n",
      "epoch 61 and loss 0.6052967309951782\n",
      "epoch 62 and loss 0.6052381992340088\n",
      "epoch 63 and loss 0.6087148189544678\n",
      "epoch 64 and loss 0.6089857220649719\n",
      "epoch 65 and loss 0.6105501055717468\n",
      "epoch 66 and loss 0.6093286275863647\n",
      "epoch 67 and loss 0.6104658246040344\n",
      "epoch 68 and loss 0.6089742183685303\n",
      "epoch 69 and loss 0.6096354126930237\n",
      "epoch 70 and loss 0.6086089015007019\n",
      "epoch 71 and loss 0.6081840991973877\n",
      "epoch 72 and loss 0.6041625142097473\n",
      "epoch 73 and loss 0.6041472554206848\n",
      "epoch 74 and loss 0.6047260165214539\n",
      "epoch 75 and loss 0.6080807447433472\n",
      "epoch 76 and loss 0.6084849238395691\n",
      "epoch 77 and loss 0.6074544191360474\n",
      "epoch 78 and loss 0.6038892269134521\n",
      "epoch 79 and loss 0.605369508266449\n",
      "epoch 80 and loss 0.6030440926551819\n",
      "epoch 81 and loss 0.6025523543357849\n",
      "epoch 82 and loss 0.6031575798988342\n",
      "epoch 83 and loss 0.6036559343338013\n",
      "epoch 84 and loss 0.6054532527923584\n",
      "epoch 85 and loss 0.6144871711730957\n",
      "epoch 86 and loss 0.6181268095970154\n",
      "epoch 87 and loss 0.6401553153991699\n",
      "epoch 88 and loss 0.6497266888618469\n",
      "epoch 89 and loss 0.6577436327934265\n",
      "epoch 90 and loss 0.6491097807884216\n",
      "epoch 91 and loss 0.6365331411361694\n",
      "epoch 92 and loss 0.6409270763397217\n",
      "epoch 93 and loss 0.6238606572151184\n",
      "epoch 94 and loss 0.6272152662277222\n",
      "epoch 95 and loss 0.6235905289649963\n",
      "epoch 96 and loss 0.6300416588783264\n",
      "epoch 97 and loss 0.6363821625709534\n",
      "epoch 98 and loss 0.6375778913497925\n",
      "epoch 99 and loss 0.6211065649986267\n",
      "epoch 100 and loss 0.6221866011619568\n",
      "epoch 101 and loss 0.6311615109443665\n",
      "epoch 102 and loss 0.639880359172821\n",
      "epoch 103 and loss 0.6271145939826965\n",
      "epoch 104 and loss 0.6320496201515198\n",
      "epoch 105 and loss 0.6394288539886475\n",
      "epoch 106 and loss 0.6450818181037903\n",
      "epoch 107 and loss 0.6458842754364014\n",
      "epoch 108 and loss 0.6523666977882385\n",
      "epoch 109 and loss 0.6572644114494324\n",
      "epoch 110 and loss 0.6472123861312866\n",
      "epoch 111 and loss 0.6296406984329224\n",
      "epoch 112 and loss 0.6363186836242676\n",
      "epoch 113 and loss 0.6113924980163574\n",
      "epoch 114 and loss 0.609239399433136\n",
      "epoch 115 and loss 0.6073756217956543\n",
      "epoch 116 and loss 0.6059361696243286\n",
      "epoch 117 and loss 0.6072319746017456\n",
      "epoch 118 and loss 0.6052706837654114\n",
      "epoch 119 and loss 0.6065072417259216\n",
      "epoch 120 and loss 0.6049997806549072\n",
      "epoch 121 and loss 0.6042590737342834\n",
      "epoch 122 and loss 0.6039236783981323\n",
      "epoch 123 and loss 0.6031153202056885\n",
      "epoch 124 and loss 0.6023193597793579\n",
      "epoch 125 and loss 0.6026150584220886\n",
      "epoch 126 and loss 0.6032483577728271\n",
      "epoch 127 and loss 0.6051003932952881\n",
      "epoch 128 and loss 0.6171213388442993\n",
      "epoch 129 and loss 0.6208446025848389\n",
      "epoch 130 and loss 0.6375634074211121\n",
      "epoch 131 and loss 0.6513539552688599\n",
      "epoch 132 and loss 0.680061399936676\n",
      "epoch 133 and loss 0.6430258750915527\n",
      "epoch 134 and loss 0.6424424052238464\n",
      "epoch 135 and loss 0.6364875435829163\n",
      "epoch 136 and loss 0.6294755339622498\n",
      "epoch 137 and loss 0.6296594142913818\n",
      "epoch 138 and loss 0.6194006204605103\n",
      "epoch 139 and loss 0.617546558380127\n",
      "epoch 140 and loss 0.6205947399139404\n",
      "epoch 141 and loss 0.6225622892379761\n",
      "epoch 142 and loss 0.6472393274307251\n",
      "epoch 143 and loss 0.6391564011573792\n",
      "epoch 144 and loss 0.6409549117088318\n",
      "epoch 145 and loss 0.6413384079933167\n",
      "epoch 146 and loss 0.6195350289344788\n",
      "epoch 147 and loss 0.6208537220954895\n",
      "epoch 148 and loss 0.624725878238678\n",
      "epoch 149 and loss 0.6271864175796509\n",
      "epoch 150 and loss 0.6268314123153687\n",
      "epoch 151 and loss 0.6302242279052734\n",
      "epoch 152 and loss 0.6368929743766785\n",
      "epoch 153 and loss 0.6417499780654907\n",
      "epoch 154 and loss 0.6278687715530396\n",
      "epoch 155 and loss 0.6296179294586182\n",
      "epoch 156 and loss 0.6355993151664734\n",
      "epoch 157 and loss 0.6350471377372742\n",
      "epoch 158 and loss 0.6429952383041382\n",
      "epoch 159 and loss 0.644729495048523\n",
      "epoch 160 and loss 0.6316822171211243\n",
      "epoch 161 and loss 0.6381948590278625\n",
      "epoch 162 and loss 0.6231935024261475\n",
      "epoch 163 and loss 0.6266791820526123\n",
      "epoch 164 and loss 0.6238695979118347\n",
      "epoch 165 and loss 0.6274564266204834\n",
      "epoch 166 and loss 0.622870922088623\n",
      "epoch 167 and loss 0.6278761625289917\n",
      "epoch 168 and loss 0.6346553564071655\n",
      "epoch 169 and loss 0.6377161145210266\n",
      "epoch 170 and loss 0.6237738132476807\n",
      "epoch 171 and loss 0.6273981928825378\n",
      "epoch 172 and loss 0.6238728165626526\n",
      "epoch 173 and loss 0.6288884878158569\n",
      "epoch 174 and loss 0.6358867883682251\n",
      "epoch 175 and loss 0.6381810903549194\n",
      "epoch 176 and loss 0.6241630911827087\n",
      "epoch 177 and loss 0.6275444626808167\n",
      "epoch 178 and loss 0.623111367225647\n",
      "epoch 179 and loss 0.6279515624046326\n",
      "epoch 180 and loss 0.634446918964386\n",
      "epoch 181 and loss 0.6378697156906128\n",
      "epoch 182 and loss 0.6238539218902588\n",
      "epoch 183 and loss 0.6271764636039734\n",
      "epoch 184 and loss 0.6235893368721008\n",
      "epoch 185 and loss 0.6284178495407104\n",
      "epoch 186 and loss 0.6352210640907288\n",
      "epoch 187 and loss 0.6375110745429993\n",
      "epoch 188 and loss 0.6228089928627014\n",
      "epoch 189 and loss 0.6233620643615723\n",
      "epoch 190 and loss 0.6134430170059204\n",
      "epoch 191 and loss 0.6085915565490723\n",
      "epoch 192 and loss 0.6053202748298645\n",
      "epoch 193 and loss 0.6044731736183167\n",
      "epoch 194 and loss 0.6051949858665466\n",
      "epoch 195 and loss 0.6035481691360474\n",
      "epoch 196 and loss 0.6036811470985413\n",
      "epoch 197 and loss 0.6037550568580627\n",
      "epoch 198 and loss 0.6063405275344849\n",
      "epoch 199 and loss 0.607230007648468\n",
      "epoch 200 and loss 0.6140753030776978\n",
      "epoch 201 and loss 0.6184971332550049\n",
      "epoch 202 and loss 0.6302924752235413\n",
      "epoch 203 and loss 0.6400822401046753\n",
      "epoch 204 and loss 0.6424927115440369\n",
      "epoch 205 and loss 0.6441797018051147\n",
      "epoch 206 and loss 0.6441482305526733\n",
      "epoch 207 and loss 0.6492179036140442\n",
      "epoch 208 and loss 0.6499404907226562\n",
      "epoch 209 and loss 0.6493237018585205\n",
      "epoch 210 and loss 0.6475576758384705\n",
      "epoch 211 and loss 0.6449333429336548\n",
      "epoch 212 and loss 0.628982424736023\n",
      "epoch 213 and loss 0.6347784996032715\n",
      "epoch 214 and loss 0.6101359128952026\n",
      "epoch 215 and loss 0.6057111024856567\n",
      "epoch 216 and loss 0.6066374778747559\n",
      "epoch 217 and loss 0.6046193838119507\n",
      "epoch 218 and loss 0.6043998599052429\n",
      "epoch 219 and loss 0.6043524146080017\n",
      "epoch 220 and loss 0.6051925420761108\n",
      "epoch 221 and loss 0.6042214632034302\n",
      "epoch 222 and loss 0.6046226024627686\n",
      "epoch 223 and loss 0.603242039680481\n",
      "epoch 224 and loss 0.6023200154304504\n",
      "epoch 225 and loss 0.6024376749992371\n",
      "epoch 226 and loss 0.6029919385910034\n",
      "epoch 227 and loss 0.6043946743011475\n",
      "epoch 228 and loss 0.6102834343910217\n",
      "epoch 229 and loss 0.6088247895240784\n",
      "epoch 230 and loss 0.6108523607254028\n",
      "epoch 231 and loss 0.6083280444145203\n",
      "epoch 232 and loss 0.6084297895431519\n",
      "epoch 233 and loss 0.6066496968269348\n",
      "epoch 234 and loss 0.6063364744186401\n",
      "epoch 235 and loss 0.6034311652183533\n",
      "epoch 236 and loss 0.6040164232254028\n",
      "epoch 237 and loss 0.6042311191558838\n",
      "epoch 238 and loss 0.6111508011817932\n",
      "epoch 239 and loss 0.6088148951530457\n",
      "epoch 240 and loss 0.6067047119140625\n",
      "epoch 241 and loss 0.6028493642807007\n",
      "epoch 242 and loss 0.6045202612876892\n",
      "epoch 243 and loss 0.6023840308189392\n",
      "epoch 244 and loss 0.601779043674469\n",
      "epoch 245 and loss 0.6021478772163391\n",
      "epoch 246 and loss 0.6026520133018494\n",
      "epoch 247 and loss 0.6032055020332336\n",
      "epoch 248 and loss 0.6049041152000427\n",
      "epoch 249 and loss 0.6120995283126831\n",
      "epoch 250 and loss 0.6086854338645935\n",
      "epoch 251 and loss 0.6074564456939697\n",
      "epoch 252 and loss 0.6065840125083923\n",
      "epoch 253 and loss 0.6057747006416321\n",
      "epoch 254 and loss 0.6028668880462646\n",
      "epoch 255 and loss 0.6060337424278259\n",
      "epoch 256 and loss 0.6023471355438232\n",
      "epoch 257 and loss 0.6015849709510803\n",
      "epoch 258 and loss 0.602127194404602\n",
      "epoch 259 and loss 0.6030333638191223\n",
      "epoch 260 and loss 0.6060492396354675\n",
      "epoch 261 and loss 0.6094760298728943\n",
      "epoch 262 and loss 0.6118571162223816\n",
      "epoch 263 and loss 0.6100480556488037\n",
      "epoch 264 and loss 0.6113015413284302\n",
      "epoch 265 and loss 0.6091874241828918\n",
      "epoch 266 and loss 0.6099250912666321\n",
      "epoch 267 and loss 0.608488142490387\n",
      "epoch 268 and loss 0.6083253622055054\n",
      "epoch 269 and loss 0.6074885725975037\n",
      "epoch 270 and loss 0.6067063212394714\n",
      "epoch 271 and loss 0.6028319001197815\n",
      "epoch 272 and loss 0.6037090420722961\n",
      "epoch 273 and loss 0.6044019460678101\n",
      "epoch 274 and loss 0.6110131740570068\n",
      "epoch 275 and loss 0.6073229908943176\n",
      "epoch 276 and loss 0.6056984663009644\n",
      "epoch 277 and loss 0.6021572947502136\n",
      "epoch 278 and loss 0.6013998985290527\n",
      "epoch 279 and loss 0.6019630432128906\n",
      "epoch 280 and loss 0.6028836965560913\n",
      "epoch 281 and loss 0.6059496998786926\n",
      "epoch 282 and loss 0.6090154051780701\n",
      "epoch 283 and loss 0.6133696436882019\n",
      "epoch 284 and loss 0.6099656820297241\n",
      "epoch 285 and loss 0.6111512184143066\n",
      "epoch 286 and loss 0.6089186072349548\n",
      "epoch 287 and loss 0.6092128753662109\n",
      "epoch 288 and loss 0.6083347797393799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 289 and loss 0.609000563621521\n",
      "epoch 290 and loss 0.6083877682685852\n",
      "epoch 291 and loss 0.6091116070747375\n",
      "epoch 292 and loss 0.6085664629936218\n",
      "epoch 293 and loss 0.6101737022399902\n",
      "epoch 294 and loss 0.6088578701019287\n",
      "epoch 295 and loss 0.6106058955192566\n",
      "epoch 296 and loss 0.6089676022529602\n",
      "epoch 297 and loss 0.6099970936775208\n",
      "epoch 298 and loss 0.6085108518600464\n",
      "epoch 299 and loss 0.6092830300331116\n",
      "epoch 300 and loss 0.608579695224762\n",
      "epoch 301 and loss 0.6090676784515381\n",
      "epoch 302 and loss 0.6085313558578491\n",
      "epoch 303 and loss 0.6102310419082642\n",
      "epoch 304 and loss 0.6088473796844482\n",
      "epoch 305 and loss 0.610633134841919\n",
      "epoch 306 and loss 0.6093283891677856\n",
      "epoch 307 and loss 0.611152172088623\n",
      "epoch 308 and loss 0.6091207265853882\n",
      "epoch 309 and loss 0.6098626852035522\n",
      "epoch 310 and loss 0.6084100008010864\n",
      "epoch 311 and loss 0.6092211008071899\n",
      "epoch 312 and loss 0.6084569096565247\n",
      "epoch 313 and loss 0.6090452075004578\n",
      "epoch 314 and loss 0.6085265874862671\n",
      "epoch 315 and loss 0.6102725267410278\n",
      "epoch 316 and loss 0.6089152097702026\n",
      "epoch 317 and loss 0.6108627915382385\n",
      "epoch 318 and loss 0.6095396280288696\n",
      "epoch 319 and loss 0.6121223568916321\n",
      "epoch 320 and loss 0.608670175075531\n",
      "epoch 321 and loss 0.6083484292030334\n",
      "epoch 322 and loss 0.6077691912651062\n",
      "epoch 323 and loss 0.6076864004135132\n",
      "epoch 324 and loss 0.6075818538665771\n",
      "epoch 325 and loss 0.6083552241325378\n",
      "epoch 326 and loss 0.6083011627197266\n",
      "epoch 327 and loss 0.6102423667907715\n",
      "epoch 328 and loss 0.6092087030410767\n",
      "epoch 329 and loss 0.6116102337837219\n",
      "epoch 330 and loss 0.6096575260162354\n",
      "epoch 331 and loss 0.6119555234909058\n",
      "epoch 332 and loss 0.6091248989105225\n",
      "epoch 333 and loss 0.6098507642745972\n",
      "epoch 334 and loss 0.6082854270935059\n",
      "epoch 335 and loss 0.6081956624984741\n",
      "epoch 336 and loss 0.6079148054122925\n",
      "epoch 337 and loss 0.6087202429771423\n",
      "epoch 338 and loss 0.6084869503974915\n",
      "epoch 339 and loss 0.6104849576950073\n",
      "epoch 340 and loss 0.6091835498809814\n",
      "epoch 341 and loss 0.6114887595176697\n",
      "epoch 342 and loss 0.6095003485679626\n",
      "epoch 343 and loss 0.6111655235290527\n",
      "epoch 344 and loss 0.6091170907020569\n",
      "epoch 345 and loss 0.6109328269958496\n",
      "epoch 346 and loss 0.608911395072937\n",
      "epoch 347 and loss 0.60979163646698\n",
      "epoch 348 and loss 0.608375072479248\n",
      "epoch 349 and loss 0.6080668568611145\n",
      "epoch 350 and loss 0.6088563203811646\n",
      "epoch 351 and loss 0.6156232953071594\n",
      "epoch 352 and loss 0.6242074370384216\n",
      "epoch 353 and loss 0.6717079281806946\n",
      "epoch 354 and loss 0.6483286619186401\n",
      "epoch 355 and loss 0.6299640536308289\n",
      "epoch 356 and loss 0.6352354288101196\n",
      "epoch 357 and loss 0.6432914733886719\n",
      "epoch 358 and loss 0.6478026509284973\n",
      "epoch 359 and loss 0.6512566208839417\n",
      "epoch 360 and loss 0.6473040580749512\n",
      "epoch 361 and loss 0.6470008492469788\n",
      "epoch 362 and loss 0.6468697786331177\n",
      "epoch 363 and loss 0.6460114121437073\n",
      "epoch 364 and loss 0.64821457862854\n",
      "epoch 365 and loss 0.6499544978141785\n",
      "epoch 366 and loss 0.6450287699699402\n",
      "epoch 367 and loss 0.6288794875144958\n",
      "epoch 368 and loss 0.6357873678207397\n",
      "epoch 369 and loss 0.6115116477012634\n",
      "epoch 370 and loss 0.6083610653877258\n",
      "epoch 371 and loss 0.606792151927948\n",
      "epoch 372 and loss 0.6037636995315552\n",
      "epoch 373 and loss 0.6050692796707153\n",
      "epoch 374 and loss 0.6037390232086182\n",
      "epoch 375 and loss 0.603358805179596\n",
      "epoch 376 and loss 0.6029930710792542\n",
      "epoch 377 and loss 0.6025474071502686\n",
      "epoch 378 and loss 0.6024143099784851\n",
      "epoch 379 and loss 0.6018213033676147\n",
      "epoch 380 and loss 0.6011406183242798\n",
      "epoch 381 and loss 0.6011591553688049\n",
      "epoch 382 and loss 0.6011280417442322\n",
      "epoch 383 and loss 0.6034190654754639\n",
      "epoch 384 and loss 0.6100596189498901\n",
      "epoch 385 and loss 0.608810305595398\n",
      "epoch 386 and loss 0.6128262877464294\n",
      "epoch 387 and loss 0.6067206859588623\n",
      "epoch 388 and loss 0.6062947511672974\n",
      "epoch 389 and loss 0.6026214361190796\n",
      "epoch 390 and loss 0.601126492023468\n",
      "epoch 391 and loss 0.6011016368865967\n",
      "epoch 392 and loss 0.6017340421676636\n",
      "epoch 393 and loss 0.6026715040206909\n",
      "epoch 394 and loss 0.6091797351837158\n",
      "epoch 395 and loss 0.6080743670463562\n",
      "epoch 396 and loss 0.6082205176353455\n",
      "epoch 397 and loss 0.6082481145858765\n",
      "epoch 398 and loss 0.6084136962890625\n",
      "epoch 399 and loss 0.6082592010498047\n",
      "epoch 400 and loss 0.6084949374198914\n",
      "epoch 401 and loss 0.6088724732398987\n",
      "epoch 402 and loss 0.6113914251327515\n",
      "epoch 403 and loss 0.609546959400177\n",
      "epoch 404 and loss 0.6120391488075256\n",
      "epoch 405 and loss 0.61268550157547\n",
      "epoch 406 and loss 0.6189412474632263\n",
      "epoch 407 and loss 0.6260430216789246\n",
      "epoch 408 and loss 0.6399978995323181\n",
      "epoch 409 and loss 0.6412786841392517\n",
      "epoch 410 and loss 0.6434093117713928\n",
      "epoch 411 and loss 0.651118814945221\n",
      "epoch 412 and loss 0.6617934107780457\n",
      "epoch 413 and loss 0.6504226326942444\n",
      "epoch 414 and loss 0.6460903286933899\n",
      "epoch 415 and loss 0.6437235474586487\n",
      "epoch 416 and loss 0.6249686479568481\n",
      "epoch 417 and loss 0.6275551915168762\n",
      "epoch 418 and loss 0.6218057870864868\n",
      "epoch 419 and loss 0.6273438930511475\n",
      "epoch 420 and loss 0.6233599781990051\n",
      "epoch 421 and loss 0.6328716278076172\n",
      "epoch 422 and loss 0.6443290710449219\n",
      "epoch 423 and loss 0.6480830907821655\n",
      "epoch 424 and loss 0.6533272862434387\n",
      "epoch 425 and loss 0.6452251076698303\n",
      "epoch 426 and loss 0.6289615631103516\n",
      "epoch 427 and loss 0.6351597309112549\n",
      "epoch 428 and loss 0.645137369632721\n",
      "epoch 429 and loss 0.6484330892562866\n",
      "epoch 430 and loss 0.6505793929100037\n",
      "epoch 431 and loss 0.6479814648628235\n",
      "epoch 432 and loss 0.6492550373077393\n",
      "epoch 433 and loss 0.6457908749580383\n",
      "epoch 434 and loss 0.6294550895690918\n",
      "epoch 435 and loss 0.6338727474212646\n",
      "epoch 436 and loss 0.6268881559371948\n",
      "epoch 437 and loss 0.6345966458320618\n",
      "epoch 438 and loss 0.618212103843689\n",
      "epoch 439 and loss 0.619278073310852\n",
      "epoch 440 and loss 0.6330599188804626\n",
      "epoch 441 and loss 0.6368332505226135\n",
      "epoch 442 and loss 0.620738685131073\n",
      "epoch 443 and loss 0.6195048093795776\n",
      "epoch 444 and loss 0.6258970499038696\n",
      "epoch 445 and loss 0.6274927258491516\n",
      "epoch 446 and loss 0.6236399412155151\n",
      "epoch 447 and loss 0.6291992664337158\n",
      "epoch 448 and loss 0.6340694427490234\n",
      "epoch 449 and loss 0.6364269256591797\n",
      "epoch 450 and loss 0.6200750470161438\n",
      "epoch 451 and loss 0.6208289265632629\n",
      "epoch 452 and loss 0.6090972423553467\n",
      "epoch 453 and loss 0.6069663166999817\n",
      "epoch 454 and loss 0.6035924553871155\n",
      "epoch 455 and loss 0.6033728718757629\n",
      "epoch 456 and loss 0.602167010307312\n",
      "epoch 457 and loss 0.6031011343002319\n",
      "epoch 458 and loss 0.6023213267326355\n",
      "epoch 459 and loss 0.6015307903289795\n",
      "epoch 460 and loss 0.6051971912384033\n",
      "epoch 461 and loss 0.6021612286567688\n",
      "epoch 462 and loss 0.605908215045929\n",
      "epoch 463 and loss 0.6017352938652039\n",
      "epoch 464 and loss 0.6008550524711609\n",
      "epoch 465 and loss 0.6014390587806702\n",
      "epoch 466 and loss 0.60154128074646\n",
      "epoch 467 and loss 0.6026847958564758\n",
      "epoch 468 and loss 0.6080984473228455\n",
      "epoch 469 and loss 0.6062510013580322\n",
      "epoch 470 and loss 0.6049556732177734\n",
      "epoch 471 and loss 0.6012805104255676\n",
      "epoch 472 and loss 0.600379467010498\n",
      "epoch 473 and loss 0.6003773212432861\n",
      "epoch 474 and loss 0.600921630859375\n",
      "epoch 475 and loss 0.6014365553855896\n",
      "epoch 476 and loss 0.6026613712310791\n",
      "epoch 477 and loss 0.6086076498031616\n",
      "epoch 478 and loss 0.6063768863677979\n",
      "epoch 479 and loss 0.6053253412246704\n",
      "epoch 480 and loss 0.6011924147605896\n",
      "epoch 481 and loss 0.6004765629768372\n",
      "epoch 482 and loss 0.6015734672546387\n",
      "epoch 483 and loss 0.6039012670516968\n",
      "epoch 484 and loss 0.6016319990158081\n",
      "epoch 485 and loss 0.602985680103302\n",
      "epoch 486 and loss 0.6030571460723877\n",
      "epoch 487 and loss 0.6082000136375427\n",
      "epoch 488 and loss 0.6057521104812622\n",
      "epoch 489 and loss 0.605902373790741\n",
      "epoch 490 and loss 0.6030954718589783\n",
      "epoch 491 and loss 0.6080519556999207\n",
      "epoch 492 and loss 0.6053088307380676\n",
      "epoch 493 and loss 0.6041341423988342\n",
      "epoch 494 and loss 0.6011421084403992\n",
      "epoch 495 and loss 0.6005879640579224\n",
      "epoch 496 and loss 0.6013296246528625\n",
      "epoch 497 and loss 0.6015337109565735\n",
      "epoch 498 and loss 0.6025802493095398\n",
      "epoch 499 and loss 0.6046091318130493\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,500):\n",
    "        pred = model.forward(X_train)\n",
    "        loss = criteria(pred,Y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        print(f\"epoch {i} and loss {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([124, 3]), torch.Size([124]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(X_train).shape,Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.FloatTensor(x_test)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "ans = model.forward(x_test).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17,  2,  1],\n",
       "       [ 0, 17,  5],\n",
       "       [ 0,  6,  6]], dtype=int64)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(np.argmax(ans,axis=1),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7407407407407407"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(17+17+6)/(25+17+12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
